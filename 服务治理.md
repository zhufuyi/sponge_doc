
sponge生成的服务代码包括了常用的服务治理功能，例如日志、链路跟踪、监控、注册中心、限流和熔断等，有的服务治理功能默认是关闭的，根据实际需要在统一在配置文件打开相应的服务治理开关。当然也支持自己实现的服务治理功能，只要封装成gin中间件或rpc拦截器即可。

### 日志

日志插件默认是开启的，默认是输出到终端，默认输出日志格式是console，可以设置输出格式为json，设置日志保存到指定文件，日志文件切割和保留时间。

在配置文件里的字段`logger`设置：

```yaml
# logger 设置
logger:
  level: "info"             # 输出日志级别 debug, info, warn, error，默认是debug
  format: "console"     # 输出格式，console或json，默认是console
  isSave: false           # false:输出到终端，true:输出到文件，默认是false
  logFileConfig:          # isSave=true时有效
    filename: "out.log"            # 文件名称，默认值out.log
    maxSize: 20                     # 最大文件大小(MB)，默认值10MB
    maxBackups: 50               # 保留旧文件的最大个数，默认值100个
    maxAge: 15                     # 保留旧文件的最大天数，默认值30天
    isCompression: true          # 是否压缩/归档旧文件，默认值false
```

<br>

### 链路跟踪

链路跟踪插件默认是关闭的，链路跟踪依赖jaeger服务。

在配置文件里的字段`enableTrace`设置：

```yaml
  enableTrace: false    # 是否开启追踪，true:启用，false:关闭，如果是true，必须设置jaeger配置。
  tracingSamplingRate: 1.0      # 链路跟踪采样率, 范围0~1.0浮点数, 0表示不采样, 1.0表示采样所有链路


# jaeger 设置
jaeger:
  agentHost: "192.168.3.37"
  agentPort: 6831
```

<br>

#### 启动jaeger和elasticsearch服务

链路跟踪使用jaeger，存储使用elasticsearch，在本地使用[docker-compose](https://github.com/docker/compose/releases)启动两个服务。

**(1) elasticsearch服务**

这是 [elasticsearch服务的启动脚本](https://github.com/zhufuyi/sponge/tree/main/test/server/elasticsearch)，**.env**文件是elasticsearch的启动配置，启动elasticsearch服务：

> docker-compose up -d

<br>

**(2) jaeger服务**

这是 [jaeger服务的启动脚本](https://github.com/zhufuyi/sponge/tree/main/test/server/jaeger)，**.env**文件是配置jaeger信息，启动jaeger服务：

> docker-compose up -d

在浏览器访问jaeger查询主页 [http://localhost:16686](http://localhost:16686) 。

<br>

#### 单服务链路跟踪示例

以 **章节3.1.2** 创建的http服务代码为例，修改配置文件`configs/edusys.yml`，开启链路跟踪功能(字段enableTrace)，并且填写jaeger配置信息。

如果想跟踪redis，启用redis缓存，把缓存类型字段**cacheType**值改为redis，并配置redis配置，同时在本地使用docker启动redis服务，这是[redis服务启动脚本](https://github.com/zhufuyi/sponge/tree/main/test/server/redis)。

启动http服务：

```bash
# 编译和运行服务
make run
```

复制 [http://localhost:8080/swagger/index.html](http://localhost:8080/apis/swagger/index.html) 到浏览器访问swagger主页，以请求get查询为例，连续请求同一个id两次，链路跟踪如图5-1所示。

![one-server-trace](https://raw.githubusercontent.com/zhufuyi/sponge_doc/main/assets/img/one-server-trace.jpg)
*图5-1 单服务链路跟踪页面*

<br>

从图中可以看到第一次请求有4个span，分别是：

- 请求接口 /api/v1/teacher/1
- 查询redis
- 查询mysql
- 设置redis缓存

说明第一次请求从redis查找，没有命中缓存，然后从mysql读取数据，最后置缓存。

第二次请求只有2个span，分别是：

- 请求接口 /api/v1/teacher/1
- 查询redis

说明第二次请求直接命中缓存，比第一次少了查询mysql和设置缓存过程。

这些span是自动生成的，很多时候需要手动添加自定义span，添加span示例：

```go
import "github.com/zhufuyi/sponge/pkg/tracer"

tags := map[string]interface{}{"foo": "bar"}
_, span := tracer.NewSpan(ctx, "spanName", tags)  
defer span.End()
```

<br>

#### 多服务链路跟踪示例

以 **章节4.3** 生成的rpc gateway服务代码为例，一个共四个服务**shopgw**、**product**、**inventory**、**comment**，分别修改4个服务配置(在configs目录下)，开启链路跟踪功能，并且填写jaeger配置信息。

在 **product**、**inventory**、**comment** 三个服务的**internal/service**目录下找到模板文件，填充代码替代`panic("implement me")`，使得代码可以正常执行，并且手动添加一个**span**，添加随机延时。

启动 **shopgw**、**product**、**inventory**、**comment** 四个服务，在浏览器访问 [http://localhost:8080/apis/swagger/index.html](http://localhost:8080/apis/swagger/index.html) ，执行get请求，链路跟踪界面如图5-2所示。

![multi-servers-trace](https://raw.githubusercontent.com/zhufuyi/sponge_doc/main/assets/img/multi-servers-trace.jpg)
*图5-2 多服务链路跟踪页面*

<br>

从图中可以看到共有10个span，主要链路：

- 请求接口/api/v1/detail
- shopgw 服务调用product的rpc客户端
- product 的rpc服务端
- product 服务中手动添加的mockDAO
- shopgw 服务调用inventory的rpc客户端
- inventory 的rpc服务端
- inventory 服务中手动添加的mockDAO
- shopgw 服务调用comment的rpc客户端
- comment 的rpc服务端
- comment 服务中手动添加的mockDAO

shopgw服务串行调用了**product**、**inventory**、**comment** 三个服务获取数据，实际中可以改为并行调用会更节省时间，但是要注意控制协程数量。

<br>

### 监控

指标采集功能默认是开启的，提供给prometheus采集数据，默认路由是`/metrics`。

在配置文件里的字段`enableMetrics`设置：

```yaml
  enableMetrics: true    # 是否开启指标采集，true：启用，false：关闭
```

#### 启动Prometheus和Grafana服务

采集指标用[Prometheus](https://prometheus.io/docs/introduction/overview)，展示使用[Grafana](https://grafana.com/docs/)，在本地使用docker启动两个服务。

**(1) prometheus服务**

这是 [prometheus服务启动脚本](https://github.com/zhufuyi/sponge/tree/main/test/server/monitor/prometheus)，启动prometheus服务：

> docker-compose up -d

在浏览器访问prometheus主页 [http://localhost:9090](http://localhost:9090/) 。

<br>

**(2) grafana服务**

这是 [grafana服务启动脚本](https://github.com/zhufuyi/sponge/tree/main/test/server/monitor/grafana)，启动grafana服务：

> docker-compose up -d

在浏览器访问 grafana 主页面 [http://localhost:33000](http://localhost:33000) ，设置prometheus的数据源 `http://localhost:9090` ，记住prometheus的数据源名称(这里是**Prometheus**)，后面导入监控面板的json的**datasource**值要一致。

<br>

#### http服务监控

以 **章节3.1.2** 生成的http服务代码为例，默认提供指标接口 [http://localhost:8080/metrics](http://localhost:8080/metrics) 。

**(1) 在prometheus添加监控目标**

打开prometheus配置文件 prometheus.yml，添加采集目标：

```bash
  - job_name: 'http-edusys'
    scrape_interval: 10s
    static_configs:
      - targets: ['localhost:8080']
```

注：如果使用vim修改 prometheus.yml 文件，修改前必须将文件 prometheus.yml 权限改为`0777`，否则修改配置文件无法同步到容器中。

执行请求使prometheus配置生效 `curl -X POST http://localhost:9090/-/reload`，稍等一会，然后在浏览器访问 [http://localhost:9090/targets](http://localhost:9090/targets) ， 检查新添加的采集目标是否生效。

<br>

**(2) 在grafana添加监控面板**

把 [http 监控面板](https://github.com/zhufuyi/sponge/blob/main/pkg/gin/middleware/metrics/gin_grafana.json) 导入到grafana，如果监控界面没有数据显示，检查json里的数据源名称与grafana配置prometheus数据源名称是否一致。

<br>

**(3) 压测接口，观察监控数据**

使用[wrk](https://github.com/wg/wrk)工具压测接口

```bash
# 接口1
wrk -t2 -c10 -d10s http://192.168.3.27:8080/api/v1/teacher/1

# 接口2
wrk -t2 -c10 -d10s http://192.168.3.27:8080/api/v1/course/1
```

监控界面如图5-3所示。

![http-grafana](https://raw.githubusercontent.com/zhufuyi/sponge_doc/main/assets/img/http-grafana.jpg)
*图5-3 http 服务监控界面*

<br>

#### rpc服务监控

以 **章节4.1.1** 生成的rpc服务代码为例，默认提供指标接口 [http://localhost:8283/metrics](http://localhost:8283/metrics) 。

**(1) 在prometheus添加监控目标**

打开prometheus配置文件 prometheus.yml，添加采集目标：

```yaml
  - job_name: 'rpc-server-edusys'
    scrape_interval: 10s
    static_configs:
      - targets: ['localhost:8283']
```

执行请求使prometheus配置生效 `curl -X POST http://localhost:9090/-/reload`，稍等一会，然后在浏览器访问 [http://localhost:9090/targets](http://localhost:9090/targets)  检查新添加的采集目标是否生效。

<br>

**(2) 在grafana添加监控面板**

把 [rpc server 监控面板](https://github.com/zhufuyi/sponge/blob/main/pkg/grpc/metrics/server_grafana.json) 导入到grafana，如果监控界面没有数据显示，检查json里的数据源名称与grafana配置prometheus数据源名称是否一致。

<br>

**(3) 压测rpc方法，观察监控数据**

使用**Goland**或**VS Code**打开`internal/service/teacher_client_test.go`文件，对**Test_teacherService_methods** 或 **Test_teacherService_benchmark** 下各个方法进行测试。

监控界面如图5-4所示。
![rpc-grafana](https://raw.githubusercontent.com/zhufuyi/sponge_doc/main/assets/img/rpc-grafana.jpg)
*图5-4 rpc server监控界面*

<br>

上面是rpc服务端的监控，rpc的客户端的监控也类似，[rpc client 监控面板](https://github.com/zhufuyi/sponge/blob/main/pkg/grpc/metrics/client_grafana.json) 。

<br>

#### 在prometheus自动添加和移除监控目标

实际使用中服务数量比较多，手动添加监控目标到prometheus比较繁琐，也容易出错。prometheus支持使用consul的服务注册与发现进行动态配置，自动添加和移除监控目标。

在本地启动 consul 服务，这是 [consul 服务启动脚本](https://github.com/zhufuyi/sponge/tree/main/test/server/consul)

打开 prometheus 配置 prometheus.yml，添加consul配置：

```yaml
  - job_name: 'consul-micro-exporter'
    consul_sd_configs:
      - server: 'localhost:8500'
        services: []  
    relabel_configs:
      - source_labels: [__meta_consul_tags]
        regex: .*edusys.*
        action: keep
      - regex: __meta_consul_service_metadata_(.+)
        action: labelmap
```

执行请求使prometheus配置生效 `curl -X POST http://localhost:9090/-/reload`。

在prometheus配置好consul服务发现之后，接着把服务的地址信息推送到consul，推送信息 edusys_exporter.json 文件内容如下：

```json
{
  "ID": "edusys-exporter",
  "Name": "edusys",
  "Tags": [
    "edusys-exporter"
  ],
  "Address": "localhost",
  "Port": 8283,
  "Meta": {
    "env": "dev",
    "project": "edusys"
  },
  "EnableTagOverride": false,
  "Check": {
    "HTTP": "http://localhost:8283/metrics",
    "Interval": "10s"
  },
  "Weights": {
    "Passing": 10,
    "Warning": 1
  }
}
```

> curl -XPUT --data @edusys_exporter.json http://localhost:8500/v1/agent/service/register

稍等一会，然后在浏览器打开 [http://localhost:9090/targets](http://localhost:9090/targets)  检查新添加的采集目标是否生效。然后关闭服务，稍等一会，检查是否自动移除采集目标。

<br>

对于自己的服务，通常启动服务时同时提交信息到consul，把 edusys_exporter.json 转为go struct，在程序内部调用http client提交给consul。

<br>

### 采集go程序profile

通常使用pprof工具来发现和定位程序问题，特别是线上go程序出现问题时可以自动把程序运行现场(profile)保存下来，再使用工具pprof分析定位问题。

sponge生成的服务支持 **http接口** 和 **系统信号通知** 两种方式采集profile，默认开启系统信号通知方式。

对于web服务，**http接口** 性能分析插件默认是关闭的，采集profile的默认路由是`/debug/pprof`，除了支持go语言本身提供默认的profile分析，还支持io分析，路由是`/debug/pprof/profile-io`。

在配置文件里的字段`enableHTTPProfile`设置：

```yaml
  enableHTTPProfile: false    # 是否开启性能分析，true：启用，false：关闭
```

<br>

#### 通过http采集profile

通过http接口方式采集profile默认是关闭的，如果需要开启，修改配置里的字段`enableHTTPProfile`为true，通常在开发或测试时使用，如果线上开启会有一点点性能损耗，根据实际情况是否开启使用。

默认路由 `/debug/pprof`，结合**go tool pprof**工具，任意时刻都可以分析当前程序运行状况。

<br>

#### 通过系统信号通知采集profile

使用http接口方式，程序后台一直定时记录profile相关信息等，绝大多数时间都不会去读取这些profile，可以改进一下，只有需要的时候再开始采集profile，采集完后自动关闭，sponge生成的服务支持监听系统信号来开启和停止采集profile，默认使用了 **SIGTRAP**(5) 系统信号(linux环境建议改为SIGUSR1，windows环境不支持SIGUSR1)，发送信号给服务：

```bash
# 通过名称查看服务pid(第二列)
ps aux | grep 服务名称

# 发送信号给服务
kill -trap pid值
# kill -usr1 pid值
```

服务收到系统信号通知后，开始采集profile并保存到`/tmp/服务名称_profile`目录，默认采集时长为60秒，60秒后自动停止采集profile，如果只想采集30秒，发送第一次信号开始采集，大概30秒后发送第二次信号表示停止采集profile，类似开关。默认采集**cpu**、**memory**、**goroutine**、**block**、**mutex**、**threadcreate**六种类型profile，文件格式`日期时间_pid_服务名称_profile类型.out`，示例：

```
xxx221809_58546_edusys_cpu.out
xxx221809_58546_edusys_mem.out
xxx221809_58546_edusys_goroutine.out
xxx221809_58546_edusys_block.out
xxx221809_58546_edusys_mutex.out
xxx221809_58546_edusys_threadcreate.out
```

因为trace的profile文件相对比较大，因此默认没有采集，根据实际需要可以开启采集trace(服务启动时调用prof.EnableTrace())。

获得离线文件后，使用pprof工具使用交互式或界面方式进行分析：

```bash
# 交互式
go tool pprof [options] source

# 界面
go tool pprof -http=[host]:[port] [options] source
```

<br>

#### 自动采集profile

上面都是手动采集profile，通常都是希望出现问题时自动采集profile。sponge生成的服务默认支持自动采集profile，是结合资源统计的告警功能来实现的，告警条件：

- 记录程序的cpu使用率连续3次(默认每分钟一次)，3次平均使用率超过80%时触发告警。
- 记录程序的使用物理内存连续3次(默认每分钟一次)，3次平均占用系统内存超过80%时触发告警。
- 如果持续超过告警阈值，默认间隔15分钟发出一次告警。

触发告警时，程序内部调用kill函数发送系统信号通知采集profile，采集的profile文件保存到`/tmp/服务名_profile`目录，其实就是在**通过系统信号通知采集profile的基础**上把手动触发改为自动触发，即使在半夜程序的cpu或内存过高，第二天也可以通过分析profile来发现程序哪里造成cpu或内存过高。

注：自动采集profile不适合windows环境。

<br>

### 配置中心

sponge生成的服务默认支持[Nacos](https://nacos.io/zh-cn/docs/v2/what-is-nacos.html)配置中心，配置中心作用是对不同环境、不同服务的配置统一管理，有效的解决地静态配置的缺点。

在本地启动nacos服务，这是[nacos服务启动配置](https://github.com/zhufuyi/sponge/tree/main/test/server/nacos)，启动nacos服务之后，在浏览器打开管理界面 http://localhost:8848/nacos/index.html ，登录账号密码进入主界面。

以 **章节3.1.2** 生成的http服务代码为例使用配置中心nacos，在nacos界面创建一个名称空间`edusys`，然后新建配置，Data ID值为`edusys.yml`，Group值为`dev`，配置内容值`configs/edusys.yml`文件内容，如图5-3所示。

![nacos-config](https://raw.githubusercontent.com/zhufuyi/sponge_doc/main/assets/img/nacos-config.jpg)
*图5-3 nacos添加服务配置*

<br>

打开edusys目录下配置中心文件`configs/edusys_cc.yml`，填写nacos配置信息：

```yaml
# Generate the go struct command: sponge config --server-dir=./serverDir

# nacos settings
nacos:
  ipAddr: "192.168.3.37"    # server address
  port: 8848                      # listening port
  scheme: "http"               # http or https
  contextPath: "/nacos"     # path
  namespaceID: "ecfe0595-cae3-43a2-9e47-216dc92207f9" # namespace id
  group: "dev"                    # group name: dev, prod, test
  dataID: "edusys.yml"        # config file id
  format: "yaml"                 # configuration file type: json,yaml,toml
```

编译和启动edusys服务：

```bash
# 切换到main.go位置
cd cmd/edusys

# 编译
go build

# 运行
./edusys -enable-cc -c=../../configs/edusys_cc.yml
```

启动服务参数`-c`表示指定配置文件，参数`-enable-cc`表示从配置中心获取配置。

<br>

### 限流

限流插件默认是关闭的，自适应限流，不需要设置其他参数。

在配置文件里的字段`enableLimit`设置：

```yaml
  enableLimit: false    # 是否开启限流(自适应)，true:开启, false:关闭
```

<br>

### 熔断

熔断插件默认是关闭的，自适应熔断，支持自定义请求返回错误码(默认500和503)进行熔断，在`internal/routers/routers.go`设置。

在配置文件里的字段`enableCircuitBreaker`设置：

```yaml
  enableCircuitBreaker: false    # 是否开启熔断(自适应)，true:开启, false:关闭
```
限流和熔断使用第三方库 [aegis](https://github.com/go-kratos/aegis)，根据系统资源和错误率自适应调整，由于不同服务器的处理能力不一样，在服务器比较多时候，参数不好设置，根据服务器处理能力自适应限流和熔断，避免了每个服务手动设置参数的麻烦。

<br>

#### 资源统计

资源统计插件默认是开启的，默认每分钟统计一次并输出到日志，资源统计了包括系统和服务本身这两部分的cpu和内存相关的数据，资源统计包含了自动触发采集profile功能，当连续3次统计本服务的CPU或内存平均值，CPU或内存平均值占用系统资源超过80%时，自动触发采集profile，默认采集为60秒，采集profile保存到`/tmp/服务名称_profile目录`，从而实现自适应采集profile，比通过人工发送系统信号来采集profile又改进了一步。

在配置文件里的字段`enableHTTPProfile`设置：

```yaml
  enableStat: true    # 是否开启资源统计，true:启用，false:关闭
```

<br>
